from dotenv import load_dotenv
import os
import assemblyai as aai
# Audio recording functionality removed - web app uses browser recording
# CLI can work with uploaded audio files
AUDIO_RECORDING_AVAILABLE = False
import sys
import os
from datetime import datetime
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich import box

# Fix the fork warning
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Use relative imports for the new package structure
from ...analysis.sentiment.cli_formatter import format_sentiment_result, format_batch_results, format_error
from ...analysis.sentiment.sa_transformers import analyze_sentiment as analyze_sentiment_transformer
from ...analysis.sentiment.sa_LLM import analyze_sentiment as analyze_sentiment_llm
from ...analysis.sentiment.combined_analyzer import analyze_sentiment_combined
from ...data.db_manager import DatabaseManager
from ...data.models import AnalyzerType, Transcription, SentimentAnalysis
from ...core.config import get_config

# Load configuration
config = get_config()
dotenv_path = config.get_dotenv_path()
load_dotenv(dotenv_path)

API = os.getenv("ASSEMBLYAI_API_KEY")

console = Console()

class SpeakerManager:
	"""Manages speaker identification with global sequential numbering"""
		
	def __init__(self, db_manager):
		self.db_manager = db_manager
		self.recording_session = None
		self.speaker_cache = {}  # Cache speakers for this session
		
	def create_recording_session(self, session_name=None):
		"""Create a new recording session"""
		self.recording_session = self.db_manager.create_recording_session(session_name)
		console.print(f"[green]‚úì[/green] Created recording session: {self.recording_session.session_name}")
		return self.recording_session
		
	def get_or_create_speaker(self, assemblyai_speaker_id):
		"""Get or create a speaker with global sequential numbering"""
		if not self.recording_session:
			raise ValueError("Recording session must be created first")
		
		# Check cache first
		if assemblyai_speaker_id in self.speaker_cache:
			return self.speaker_cache[assemblyai_speaker_id]
		
		# Get or create speaker
		speaker = self.db_manager.get_or_create_speaker(
			self.recording_session.id, 
			assemblyai_speaker_id
		)
		
		# Cache the speaker
		self.speaker_cache[assemblyai_speaker_id] = speaker
		
		console.print(f"[green]‚úì[/green] Using speaker: {speaker.display_name}")
		return speaker
		
	def finalize_session(self):
		"""Finalize the recording session and update statistics"""
		if self.recording_session:
			# Store the session ID and name before any database operations
			session_id = self.recording_session.id
			session_name = self.recording_session.session_name
			
			try:
				self.db_manager.update_recording_session_stats(session_id)
				console.print(f"[green]‚úì[/green] Finalized session: {session_name}")
			except Exception as e:
				console.print(f"[yellow]‚ö†Ô∏è[/yellow] Warning: Could not update session stats: {e}")
		
	def close(self):
		"""Clean up resources"""
		self.finalize_session()
		self.speaker_cache.clear()

def record(duration=65, filename=None):
	"""Record audio functionality removed - use web browser recording or upload audio files"""
	raise RuntimeError("Direct audio recording removed. Use web interface for recording or upload audio files for analysis.")

def analyze_audio(audio_file, use_llm=True, expected_speakers=None):
	"""
	Analyze audio file using AssemblyAI and perform sentiment analysis.
		
	Args:
		audio_file (str): Path to the audio file
		use_llm (bool): Whether to use LLM-based sentiment analysis (True) or transformer-based (False)
		expected_speakers (int, optional): Expected number of speakers. If None, will be automatically detected.
		
	Returns:
		dict: Analysis results including transcription and sentiment analysis
	"""
	aai.settings.api_key = API
		
	# Initialize database manager with correct path
	config = get_config()
	db_manager = DatabaseManager(config.get_database_url())
	speaker_manager = SpeakerManager(db_manager)
	
	# Create recording session
	recording_session = speaker_manager.create_recording_session()
		
	try:
		# Configure AssemblyAI with enhanced speaker diarization settings AND content safety
		config = aai.TranscriptionConfig(
			speech_model=aai.SpeechModel.best,
			speaker_labels=True,  # Enable speaker diarization
			punctuate=True,       # Add punctuation
			format_text=True,     # Format text for better readability
			# Enhanced speaker diarization settings
			speakers_expected=expected_speakers,  # Use the expected_speakers parameter
			# Additional quality improvements
			auto_chapters=False,  # Disable to avoid interference with speaker detection
			word_boost=["speaker", "person", "voice"],  # Boost speaker-related words
			boost_param="high",    # High boost for better speaker detection
			# ENHANCED: Content safety and filtering
			filter_profanity=True,  # Filter profanity with asterisks
			content_safety=True,    # Enable content safety detection
			content_safety_confidence=75  # Set confidence threshold to 75%
		)

		console.print("\n[bold]Transcribing audio with speaker diarization and content safety...[/bold]")
		transcript = aai.Transcriber(config=config).transcribe(audio_file)

		if transcript.status == "error":
			raise RuntimeError(f"Transcription failed: {transcript.error}")

		# ENHANCED: Check for empty or no-speech audio
		if not transcript.utterances or len(transcript.utterances) == 0:
			console.print(f"[bold red]‚ùå No Speech Detected[/bold red]")
			console.print("[yellow]The audio file contains no detectable speech.[/yellow]")
			console.print("[yellow]This could be due to:[/yellow]")
			console.print("[yellow]‚Ä¢ Silent audio or background noise only[/yellow]")
			console.print("[yellow]‚Ä¢ Very poor audio quality[/yellow]")
			console.print("[yellow]‚Ä¢ Non-speech audio (music, sounds, etc.)[/yellow]")
			console.print("[yellow]‚Ä¢ Audio too short or quiet[/yellow]")
			
			return {
				"utterances": [],
				"status": "no_speech",
				"message": "No speech detected in audio file",
				"suggestions": [
					"Check audio quality and volume",
					"Ensure the recording contains clear speech",
					"Try a longer recording with more content",
					"Verify the audio file is not corrupted"
				]
			}

		# Debug information about the transcript
		console.print(f"\n[bold]Transcript Status:[/bold] {transcript.status}")
		console.print(f"[bold]Number of utterances:[/bold] {len(transcript.utterances)}")
		
		# ENHANCED: Content safety analysis
		if hasattr(transcript, 'content_safety_labels') and transcript.content_safety_labels:
			content_safety = transcript.content_safety_labels
			if content_safety.status == "success" and content_safety.results:
				console.print(f"\n[bold red]üö® Content Safety Alert[/bold red]")
				console.print(f"[red]Potentially inappropriate content detected:[/red]")
				
				for result in content_safety.results:
					for label in result.labels:
						severity_text = "Low" if label.severity < 0.3 else "Medium" if label.severity < 0.7 else "High"
						console.print(f"[red]‚Ä¢ {label.label.title()}: {label.confidence:.1%} confidence, {severity_text} severity[/red]")
						console.print(f"[red]  Text: \"{result.text[:100]}...\"[/red]")
				
				# Ask user if they want to continue
				console.print(f"\n[yellow]‚ö†Ô∏è Warning: This content may not be suitable for sentiment analysis.[/yellow]")
				console.print(f"[yellow]Content safety features have been applied (profanity filtered).[/yellow]")
		
		# Get unique speakers
		unique_speakers = set(utterance.speaker for utterance in transcript.utterances)
		console.print(f"[bold]Number of detected speakers:[/bold] {len(unique_speakers)}")
		console.print(f"[bold]Speaker IDs:[/bold] {', '.join(unique_speakers)}")
		
		# Enhanced speaker analysis - detect potential issues
		if expected_speakers and len(unique_speakers) != expected_speakers:
			console.print(f"[bold yellow]‚ö†Ô∏è Warning:[/bold yellow] Expected {expected_speakers} speakers but detected {len(unique_speakers)}")
			console.print("[yellow]Tip: For better speaker separation:[/yellow]")
			console.print("[yellow]- Ensure each speaker talks for at least 30 seconds[/yellow]")
			console.print("[yellow]- Avoid short phrases like 'yeah', 'right', 'sounds good'[/yellow]")
			console.print("[yellow]- Minimize background noise and cross-talk[/yellow]")
			console.print("[yellow]- Consider using multichannel recording if possible[/yellow]")
		
		# ENHANCED: Analyze utterance patterns for quality assessment
		short_utterances = [u for u in transcript.utterances if len(u.text.split()) < 3]
		very_short_utterances = [u for u in transcript.utterances if len(u.text.strip()) < 5]  # Less than 5 characters
		nonsensical_utterances = []
		
		# Check for nonsensical content
		for utterance in transcript.utterances:
			text = utterance.text.strip().lower()
			# Check for patterns that suggest nonsensical input
			if (len(text.split()) < 2 and 
				not any(word in text for word in ['yes', 'no', 'ok', 'okay', 'hello', 'hi', 'bye', 'thanks', 'thank you']) and
				len(text) > 0):
				nonsensical_utterances.append(utterance)
		
		# Quality warnings
		if len(short_utterances) > len(transcript.utterances) * 0.3:  # More than 30% short utterances
			console.print(f"[bold yellow]‚ö†Ô∏è Quality Warning:[/bold yellow] {len(short_utterances)} out of {len(transcript.utterances)} utterances are very short")
			console.print("[yellow]This may affect speaker diarization and sentiment analysis accuracy[/yellow]")
		
		if len(very_short_utterances) > len(transcript.utterances) * 0.5:  # More than 50% very short
			console.print(f"[bold yellow]‚ö†Ô∏è Quality Warning:[/bold yellow] {len(very_short_utterances)} out of {len(transcript.utterances)} utterances are extremely short")
			console.print("[yellow]Consider recording longer, more substantial speech for better analysis[/yellow]")
		
		if len(nonsensical_utterances) > 0:
			console.print(f"[bold yellow]‚ö†Ô∏è Content Warning:[/bold yellow] {len(nonsensical_utterances)} utterances may contain nonsensical content")
			console.print("[yellow]This could be due to:[/yellow]")
			console.print("[yellow]‚Ä¢ Background noise being interpreted as speech[/yellow]")
			console.print("[yellow]‚Ä¢ Poor audio quality causing transcription errors[/yellow]")
			console.print("[yellow]‚Ä¢ Non-speech sounds (music, effects, etc.)[/yellow]")
		
		# Process each utterance with sentiment analysis
		results = []
		processed_count = 0
		skipped_count = 0
		
		for utterance in transcript.utterances:
			speaker_id = utterance.speaker
			text = utterance.text.strip()
			
			# ENHANCED: Skip empty or very short utterances
			if not text or len(text.strip()) < 2:
				skipped_count += 1
				console.print(f"[dim]‚è≠Ô∏è Skipped empty/very short utterance[/dim]")
				continue
			
			# ENHANCED: Check for nonsensical content and handle appropriately
			if _is_nonsensical_content(text):
				console.print(f"[yellow]‚ö†Ô∏è Potentially nonsensical content detected: \"{text[:50]}...\"[/yellow]")
				console.print(f"[yellow]Proceeding with analysis but results may be unreliable[/yellow]")
				
			# Get or create speaker for this session
			speaker = speaker_manager.get_or_create_speaker(speaker_id)
			
			# DUPLICATE PREVENTION: Check if this exact text already exists
			existing_transcription = db_manager.session.query(Transcription).filter_by(text=text).first()
			if existing_transcription:
				console.print(f"[yellow]‚ö†Ô∏è[/yellow] Duplicate transcription detected: \"{text[:50]}...\"")
				console.print(f"[yellow]Skipping storage, using existing transcription ID: {existing_transcription.id}[/yellow]")
				transcription = existing_transcription
				
				# Check if sentiment analysis already exists for this transcription
				existing_analysis = db_manager.session.query(SentimentAnalysis).filter_by(transcription_id=existing_transcription.id).first()
				if existing_analysis:
					console.print(f"[yellow]‚ö†Ô∏è[/yellow] Sentiment analysis already exists, skipping analysis")
					# Use existing analysis data to create the result
					combined_sentiment = {
						'label': existing_analysis.label,
						'category': existing_analysis.category,
						'score': existing_analysis.score,
						'confidence': existing_analysis.confidence,
						'explanation': existing_analysis.explanation or 'Existing analysis',
						'has_llm': existing_analysis.analyzer_type == AnalyzerType.COMBINED,
						'analysis_source': 'existing'
					}
					skip_new_analysis = True
				else:
					skip_new_analysis = False
			else:
				# Store transcription with enhanced metadata
				transcription = db_manager.add_transcription(
					speaker.id, 
					text,
					duration=(utterance.end - utterance.start) / 1000.0,  # Convert ms to seconds
					confidence_score=getattr(utterance, 'confidence', None)
				)
				console.print(f"[blue]üíæ[/blue] Stored NEW transcription for {speaker.display_name}: {text[:50]}...")
				skip_new_analysis = False
			
			# ENHANCED: Perform combined sentiment analysis (unless we're using existing)
			if not skip_new_analysis:
				try:
					# Use combined analyzer for single, more accurate result
					combined_sentiment = analyze_sentiment_combined(
						text, speaker_id, None, use_llm=use_llm, verbose=False
					)
					processed_count += 1
					console.print(f"[green]üîÑ[/green] Combined analysis: {combined_sentiment['category']} (confidence: {combined_sentiment['confidence']:.1%})")
				except Exception as e:
					console.print(f"[red]‚ùå Combined analysis failed for utterance: {str(e)}[/red]")
					# Fallback to simple transformer analysis
					try:
						combined_sentiment = analyze_sentiment_transformer(text, speaker_id, None, verbose=False)
						console.print(f"[yellow]üîÑ[/yellow] Fallback to transformer: {combined_sentiment['category']}")
					except Exception as e2:
						console.print(f"[red]‚ùå Transformer fallback also failed: {str(e2)}[/red]")
						combined_sentiment = _create_fallback_sentiment_result(text, "all_analysis_failed")
			else:
				console.print(f"[yellow]üîÑ[/yellow] Using existing analysis: {combined_sentiment['category']} (confidence: {combined_sentiment['confidence']:.1%})")
			
			# Store ONLY the combined analysis result in database (not separate analyses)
			# Only store new analysis if we didn't skip it
			if not skip_new_analysis:
				# Determine the appropriate analyzer type based on what was actually used
				if combined_sentiment.get('has_llm', False) and combined_sentiment.get('analysis_source') not in ['transformer_only', 'fallback']:
					analyzer_type = AnalyzerType.COMBINED
					analysis_description = f"Combined analysis using transformer + LLM ({combined_sentiment.get('combination_strategy', 'weighted_average')})"
				else:
					analyzer_type = AnalyzerType.TRANSFORMER
					analysis_description = "Transformer-only analysis (LLM unavailable or failed)"
				
				combined_analysis = db_manager.add_sentiment_analysis(
					transcription_id=transcription.id,
					analyzer_type=analyzer_type,  # Use appropriate type based on actual analysis
					label=combined_sentiment['label'],
					category=combined_sentiment['category'],
					score=combined_sentiment['score'],
					confidence=combined_sentiment['confidence'],
					explanation=combined_sentiment.get('explanation', analysis_description)
				)
				console.print(f"[green]üíæ[/green] Stored NEW {analyzer_type.value} analysis: {combined_sentiment['category']}")
				
				# Store detailed metadata about the combination in the explanation field
				if 'combination_strategy' in combined_sentiment:
					detailed_explanation = f"{analysis_description}. "
					if 'transformer_category' in combined_sentiment and 'llm_category' in combined_sentiment:
						detailed_explanation += f"Transformer: {combined_sentiment['transformer_category']}, "
						detailed_explanation += f"LLM: {combined_sentiment['llm_category']}"
					combined_analysis.explanation = detailed_explanation
					db_manager.session.commit()
			else:
				console.print(f"[green]üíæ[/green] Using EXISTING analysis: {combined_sentiment['category']}")
			
			results.append({
				"speaker": speaker.display_name,
				"speaker_id": speaker.id,
				"global_sequence": speaker.global_sequence,
				"text": text,
				"start_time": utterance.start,
				"end_time": utterance.end,
				"combined_sentiment": combined_sentiment,
				"transformer_sentiment": combined_sentiment,  # For backward compatibility
				"llm_sentiment": None  # No longer storing separate LLM results
			})

		# ENHANCED: Provide processing summary
		console.print(f"\n[bold green]‚úÖ Processing Complete[/bold green]")
		console.print(f"[green]Successfully processed: {processed_count} utterances[/green]")
		if skipped_count > 0:
			console.print(f"[yellow]Skipped: {skipped_count} empty/very short utterances[/yellow]")

		return {
			"utterances": results,
			"status": "success",
			"recording_session_id": recording_session.id,
			"processing_summary": {
				"total_utterances": len(transcript.utterances),
				"processed": processed_count,
				"skipped": skipped_count,
				"quality_warnings": len(short_utterances) + len(very_short_utterances) + len(nonsensical_utterances)
			}
		}
		
	except Exception as e:
		console.print(f"[bold red]‚ùå Analysis Failed[/bold red]")
		console.print(f"[red]Error: {str(e)}[/red]")
		console.print(f"[yellow]This could be due to:[/yellow]")
		console.print(f"[yellow]‚Ä¢ Network connectivity issues[/yellow]")
		console.print(f"[yellow]‚Ä¢ Invalid audio file format[/yellow]")
		console.print(f"[yellow]‚Ä¢ AssemblyAI API issues[/yellow]")
		console.print(f"[yellow]‚Ä¢ Insufficient API credits[/yellow]")
		
		return {
			"utterances": [],
			"status": "error",
			"error": str(e),
			"suggestions": [
				"Check your internet connection",
				"Verify the audio file is valid and accessible",
				"Check your AssemblyAI API key and credits",
				"Try again in a few minutes"
			]
		}
		
	finally:
		# Finalize the session stats BEFORE closing the database connection
		speaker_manager.close()
		db_manager.close()

def _is_nonsensical_content(text: str) -> bool:
	"""
	Check if the text appears to be nonsensical or likely transcription errors.
	
	Args:
		text: The text to analyze
		
	Returns:
		bool: True if the content appears nonsensical
	"""
	text_lower = text.lower().strip()
	words = text_lower.split()
	
	# Very short single words that aren't common speech
	if len(words) == 1 and len(text_lower) < 4:
		common_short_words = ['yes', 'no', 'ok', 'hi', 'bye', 'wow', 'oh', 'ah', 'um', 'uh', 'you']
		if text_lower not in common_short_words:
			return True
	
	# Repetitive patterns (like "la la la", "tick tock tick tock")
	if len(words) >= 3:
		# Check for repetitive patterns - but only if words are actually repeated
		unique_words = set(words)
		
		# Case 1: Very few unique words repeated many times (like "la la la la")
		if len(unique_words) <= 2 and len(words) >= 4:
			return True
		
		# Case 2: Moderate repetition with simple words (like "tick tock tick tock la la la")
		if len(unique_words) <= 3 and len(words) >= 6:
			# Check if most words are repeated
			word_counts = {}
			for word in words:
				word_counts[word] = word_counts.get(word, 0) + 1
			
			# If any word appears 3+ times, it's likely nonsensical
			max_repetitions = max(word_counts.values())
			if max_repetitions >= 3:
				return True
	
	# Check for patterns that suggest noise/nonsense
	nonsense_patterns = [
		r'^[a-z]{1,3}\.{3,}$',  # Single letters with dots like "a..."
		r'^[a-z\s]{1,10}\.$',   # Very short with just a period
		r'^[^a-zA-Z\s]*$',      # No letters at all (symbols only)
		r'^[a-z]\s[a-z]\s[a-z](\s[a-z])*$',  # FIXED: Only single letters separated by spaces (like "a b c d")
	]
	
	import re
	for pattern in nonsense_patterns:
		if re.match(pattern, text_lower):
			return True
	
	# Check for excessive punctuation or symbols
	non_alpha_count = sum(1 for char in text if not char.isalpha() and not char.isspace())
	if len(text) > 0 and non_alpha_count / len(text) > 0.5:  # More than 50% non-alphabetic
		return True
	
	return False

def _create_fallback_sentiment_result(text: str, error_type: str) -> dict:
	"""
	Create a fallback sentiment result when analysis fails.
	
	Args:
		text: The original text
		error_type: Type of error that occurred
		
	Returns:
		dict: Fallback sentiment result
	"""
	return {
		"score": 0.0,
		"label": "neutral",
		"category": "reflective_neutral",
		"intensity": 0.0,
		"confidence": 0.0,
		"classification_confidence": 0.0,
		"matched_patterns": [],
		"explanation": f"Analysis failed ({error_type}). Fallback neutral classification applied for: \"{text[:50]}...\""
	}

def print_analysis(analysis):
	"""Print the analysis results using enhanced CLI formatting with detailed explanations."""
	console.print("\n[bold cyan]=== üé§ Open Mic Session Analysis ===[/bold cyan]")
	
	# ENHANCED: Handle different analysis statuses
	status = analysis.get("status", "unknown")
	
	if status == "no_speech":
		# Handle no speech detected
		console.print(f"\n[bold red]‚ùå No Speech Detected[/bold red]")
		console.print(f"[yellow]{analysis.get('message', 'No speech found in audio')}[/yellow]")
		
		if "suggestions" in analysis:
			console.print(f"\n[bold blue]üí° Suggestions:[/bold blue]")
			for suggestion in analysis["suggestions"]:
				console.print(f"[blue]‚Ä¢ {suggestion}[/blue]")
		return
	
	elif status == "error":
		# Handle analysis errors
		console.print(f"\n[bold red]‚ùå Analysis Failed[/bold red]")
		console.print(f"[red]Error: {analysis.get('error', 'Unknown error occurred')}[/red]")
		
		if "suggestions" in analysis:
			console.print(f"\n[bold blue]üí° Suggestions:[/bold blue]")
			for suggestion in analysis["suggestions"]:
				console.print(f"[blue]‚Ä¢ {suggestion}[/blue]")
		return
	
	# Handle successful analysis
	utterances = analysis.get("utterances", [])
	
	if not utterances:
		console.print(f"\n[bold yellow]‚ö†Ô∏è No Valid Utterances Found[/bold yellow]")
		console.print(f"[yellow]All utterances were filtered out or failed processing[/yellow]")
		
		# Show processing summary if available
		if "processing_summary" in analysis:
			summary = analysis["processing_summary"]
			console.print(f"\n[bold]üìä Processing Summary:[/bold]")
			console.print(f"[yellow]Total utterances detected: {summary.get('total_utterances', 0)}[/yellow]")
			console.print(f"[yellow]Successfully processed: {summary.get('processed', 0)}[/yellow]")
			console.print(f"[yellow]Skipped: {summary.get('skipped', 0)}[/yellow]")
			console.print(f"[yellow]Quality warnings: {summary.get('quality_warnings', 0)}[/yellow]")
		return
	
	# Group utterances by speaker
	speaker_utterances = {}
	for utterance in utterances:
		speaker = utterance["speaker"]
		if speaker not in speaker_utterances:
			speaker_utterances[speaker] = []
		speaker_utterances[speaker].append(utterance)
	
	# ENHANCED: Show processing summary if available
	if "processing_summary" in analysis:
		summary = analysis["processing_summary"]
		console.print(f"\n[bold green]üìä Processing Summary:[/bold green]")
		
		summary_table = Table(box=box.ROUNDED, show_header=True, header_style="bold green")
		summary_table.add_column("üìà Metric", style="cyan", width=20)
		summary_table.add_column("üéØ Value", style="green", width=10)
		summary_table.add_column("üìù Description", style="yellow")
		
		summary_table.add_row("Total Detected", str(summary.get('total_utterances', 0)), "Total utterances found by AssemblyAI")
		summary_table.add_row("Successfully Processed", str(summary.get('processed', 0)), "Utterances that completed sentiment analysis")
		summary_table.add_row("Skipped", str(summary.get('skipped', 0)), "Empty or very short utterances filtered out")
		summary_table.add_row("Quality Warnings", str(summary.get('quality_warnings', 0)), "Utterances with potential quality issues")
		
		console.print(summary_table)
	
	# Print enhanced summary table
	summary_table = Table(box=box.ROUNDED, show_header=True, header_style="bold magenta")
	summary_table.add_column("üé≠ Speaker", style="cyan", width=15)
	summary_table.add_column("üìù Contributions", style="green", width=15)
	summary_table.add_column("‚è±Ô∏è Time Range", style="yellow", width=20)
	summary_table.add_column("üîÑ Transformer", style="blue", width=15)
	summary_table.add_column("ü§ñ LLM", style="magenta", width=15)
	
	for speaker, utterances in speaker_utterances.items():
		start_time = min(u["start_time"] for u in utterances)
		end_time = max(u["end_time"] for u in utterances)
		
		# Get dominant categories
		transformer_categories = [u["transformer_sentiment"]["category"] for u in utterances]
		llm_categories = [u["llm_sentiment"]["category"] if u["llm_sentiment"] else "N/A" for u in utterances]
		
		# Most common category
		from collections import Counter
		transformer_dominant = Counter(transformer_categories).most_common(1)[0][0]
		llm_dominant = Counter([c for c in llm_categories if c != "N/A"]).most_common(1)[0][0] if any(c != "N/A" for c in llm_categories) else "N/A"
		
		summary_table.add_row(
			speaker,
			str(len(utterances)),
			f"{start_time:.1f}s - {end_time:.1f}s",
			transformer_dominant,
			llm_dominant
		)
	
	console.print("\n[bold]üìä Session Summary:[/bold]")
	console.print(summary_table)
	
	# Print detailed analysis for each speaker using enhanced formatter
	for speaker, utterances in speaker_utterances.items():
		console.print(f"\n[bold cyan]üé≠ Speaker: {speaker}[/bold cyan]")
		
		for i, utterance in enumerate(utterances, 1):
			console.print(f"\n[bold blue]üìù Contribution {i}[/bold blue]")
			
			# ENHANCED: Check for potential issues with this utterance
			text = utterance["text"]
			issues = []
			
			if len(text.strip()) < 10:
				issues.append("Very short text")
			if _is_nonsensical_content(text):
				issues.append("Potentially nonsensical content")
			if utterance["transformer_sentiment"]["confidence"] < 0.3:
				issues.append("Low confidence analysis")
			
			# Create utterance info panel with warnings if needed
			utterance_info_text = (
				f"[bold]Text:[/bold] {text}\n"
				f"[bold]Time:[/bold] {utterance['start_time']:.1f}s - {utterance['end_time']:.1f}s\n"
				f"[bold]Duration:[/bold] {utterance['end_time'] - utterance['start_time']:.1f}s\n"
				f"[bold]Session Speaker ID:[/bold] {utterance.get('session_speaker_id', 'N/A')}"
			)
			
			if issues:
				utterance_info_text += f"\n[bold red]‚ö†Ô∏è Issues:[/bold red] {', '.join(issues)}"
			
			utterance_info = Panel(
				utterance_info_text,
				title="üé§ Utterance Details",
				border_style="blue" if not issues else "yellow",
				padding=(1, 2)
			)
			console.print(utterance_info)
			
			# Show detailed transformer analysis
			console.print(f"\n[bold green]üîÑ TRANSFORMER ANALYSIS[/bold green]")
			format_sentiment_result(utterance['transformer_sentiment'])
			
			# Show detailed LLM analysis if available
			if utterance['llm_sentiment']:
				console.print(f"\n[bold yellow]ü§ñ LLM ANALYSIS[/bold yellow]")
				format_sentiment_result(utterance['llm_sentiment'])
			else:
				console.print(f"\n[bold red]ü§ñ LLM ANALYSIS: Not Available[/bold red]")
			
			# Comparison panel if both analyses are available
			if utterance['llm_sentiment']:
				transformer_cat = utterance['transformer_sentiment']['category']
				llm_cat = utterance['llm_sentiment']['category']
				
				if transformer_cat == llm_cat:
					agreement_status = f"[green]‚úÖ AGREEMENT: Both models classify as {transformer_cat.upper()}[/green]"
				else:
					agreement_status = f"[yellow]‚ö†Ô∏è DISAGREEMENT: Transformer={transformer_cat.upper()}, LLM={llm_cat.upper()}[/yellow]"
				
				comparison_panel = Panel(
					f"{agreement_status}\n\n"
					f"[bold]Score Comparison:[/bold]\n"
					f"‚Ä¢ Transformer: {utterance['transformer_sentiment']['score']:.3f}\n"
					f"‚Ä¢ LLM: {utterance['llm_sentiment']['score']:.3f}\n\n"
					f"[bold]Confidence Comparison:[/bold]\n"
					f"‚Ä¢ Transformer: {utterance['transformer_sentiment']['confidence']:.1%}\n"
					f"‚Ä¢ LLM: {utterance['llm_sentiment']['confidence']:.1%}",
					title="üîç Model Comparison",
					border_style="magenta",
					padding=(1, 2)
				)
				console.print(f"\n")
				console.print(comparison_panel)
			
			console.print("\n" + "="*100)

if __name__ == "__main__":
	# For testing: Use an existing audio file
	# analysis = analyze_audio("path/to/your/audio/file.wav", use_llm=True)
	# print_analysis(analysis)
	print("üé§ Hopes & Sorrows Audio Analysis")
	print("Use the web interface for recording or upload audio files for analysis.")
	print("Example usage:")
	print("  from hopes_sorrows.analysis.audio import analyze_audio")
	print("  analysis = analyze_audio('your_audio_file.wav')")
	print("  print_analysis(analysis)")
		
